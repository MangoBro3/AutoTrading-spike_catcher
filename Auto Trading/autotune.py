
import os
import time
import json
import uuid
import pandas as pd
import numpy as np
import random
from datetime import datetime, timedelta
import itertools
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

# Modules
from backtester import Backtester

from strategy import Strategy

class AutoTuner:
    def __init__(self, raw_dfs, base_params, output_dir="autotune_runs"):
        self.raw_dfs = raw_dfs # Changed from symbol_dfs
        self.base_params = base_params
        self.output_dir = output_dir
        
        # Determine Date Range from Data
        all_dates = set()
        for df in raw_dfs.values():
            if not df.empty and 'datetime' in df.columns:
                 all_dates.update(pd.to_datetime(df['datetime']))
        self.dates = sorted(list(all_dates))
        
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            
    def generate_trials(self, group, num_trials, seed=42):
        """
        Generate parameter sets based on Groups (A, B, C)
        Stage 1: Random Search (Uniform)
        """
        random.seed(seed)
        np.random.seed(seed)
        
        trials = []
        
        # Parameter Space Rules
        # Group A: Entry Quality
        space_A = {
            'trigger_vol_A': [1.5, 2.0, 2.5, 3.0, 4.0],
            'close_confirm_pct_A': [0.1, 0.3, 0.5, 1.0],
            'rsi_ceiling_A': [65, 70, 75, 80],
            'rsi_entry_B': [30, 35, 40, 45, 50]
        }
        
        # Group B: Exit/Risk
        space_B = {
            'sl_atr_mult_A': [1.5, 1.8, 2.0, 2.5],
            'trail_atr_mult_A': [2.0, 2.5, 3.0, 4.0],
            'partial_tp_r_A': [1.0, 1.2, 1.5, 2.0],
            'sl_atr_mult_B': [1.0, 1.2, 1.5, 2.0]
        }
        
        # Group C: Portfolio
        space_C = {
            'max_entries_per_day': [1, 2, 3],
            'max_open_positions': [3, 4, 5, 8],
            'cooldown_days_after_sl': [1, 3, 5, 7],
            'daily_loss_limit_pct': [1.0, 2.0, 3.0, 5.0]
        }
        
        target_space = {}
        if group == 'A': target_space = space_A
        elif group == 'B': target_space = space_B
        elif group == 'C': target_space = space_C
        else: return [] # Invalid group
        
        # Generate Trials
        # Strategy: Random Sampling from discrete grids
        keys = list(target_space.keys())
        
        seen_hashes = set()
        
        # Always include Base Params (Trial 0) if it belongs to group? 
        # Or just start fresh. Let's start fresh + Base.
        
        # 1. Base Param Identity (Trial 0) - Only if it matches the group?
        # Actually AutoTune assumes we are exploring.
        
        while len(trials) < num_trials:
            new_p = self.base_params.copy()
            
            # Mutate target keys
            combo_hash_parts = []
            for k in keys:
                val = random.choice(target_space[k])
                new_p[k] = val
                combo_hash_parts.append(f"{k}:{val}")
            
            combo_str = "|".join(sorted(combo_hash_parts))
            
            if combo_str not in seen_hashes:
                seen_hashes.add(combo_str)
                trials.append(new_p)
                
            # Circuit breaker
            if len(seen_hashes) >= np.prod([len(target_space[k]) for k in keys]):
                break
                
        return trials

    def calculate_min_trades(self, start_date, end_date):
        """Phase 1.1: Dynamic Min Trades"""
        if not start_date or not end_date: return 10
        days = (end_date - start_date).days
        # Formula: clamp(days/30 * k, 10, 240)
        # k=4 for now (conservative)
        required = max(10, int(days / 30 * 4))
        return min(required, 100) # Cap at 100 per fold

    def calculate_score(self, ret, max_dd, trades, win_rate):
        """Score v1.1: Profit * Stability"""
        if trades == 0: return -1.0
        
        # 1. Base: Avg Return per Trade (Log scaled to prevent luck bias)
        # But we have Total Return.
        # Let's use Total Return BUT penalized heavily if DD is bad.
        
        # Stability Factor: WinRate is good proxy. MaxDD is penalty.
        # stability = win_rate / (1 + abs(max_dd)*2)
        
        # Score = Total_Ret * Stability * log(Trades)
        # Log trades rewards meaningful sample size.
        
        stability = win_rate / (1.0 + abs(max_dd) * 5.0) # Heavy penalty on DD
        sample_bonus = np.log1p(trades)
        
        score = ret * stability * sample_bonus
        return score

    def run_process(self, group, num_trials=20, seed=42, callback=None):
        """
        Main AutoTune Process
        1. Generate Trials
        2. Walk-Forward Eval (4 Folds)
        3. Save Results
        """
        run_id = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{group}_N{num_trials}"
        run_dir = os.path.join(self.output_dir, run_id)
        os.makedirs(run_dir)
        
        # Save Config
        config = {
            'group': group,
            'num_trials': num_trials,
            'seed': seed,
            'base_params': self.base_params
        }
        with open(os.path.join(run_dir, "run_config.json"), "w") as f:
            json.dump(config, f, indent=4, default=str)
            
        # 1. Generate Trials
        trial_params_list = self.generate_trials(group, num_trials, seed)
        
        # 2. Prepare Folds
        if not self.dates:
            return None
            
        # Split dates into 4 chunks
        # Simple count-based split
        total_days = len(self.dates)
        chunk_size = total_days // 4
        
        folds = []
        for i in range(4):
            start_i = i * chunk_size
            end_i = (i + 1) * chunk_size if i < 3 else total_days
            folds.append((self.dates[start_i], self.dates[end_i-1]))
            
        # 3. Execution Loop
        results = []
        bt = Backtester()
        
        total_steps = len(trial_params_list) * 4
        current_step = 0
        
        print(f"[AutoTune] Starting Run {run_id} with {len(trial_params_list)} trials over 4 folds.")
        
        strat = Strategy()
        
        for t_idx, params in enumerate(trial_params_list):
            trial_id = f"trial_{t_idx:04d}"
            trial_dir = os.path.join(run_dir, trial_id)
            os.makedirs(trial_dir)
            
            # --- Re-Analyze Signals (CRITICAL for Parameter Tuning) ---
            # Some params (vol, rsi thresholds) change signals.
            # We must re-run analyze on raw data.
            current_symbol_dfs = {}
            for s, df in self.raw_dfs.items():
                if df.empty: continue
                # Skip stable coins if needed (usually done in app level, but safeguard here)
                if "USDT" in s or "USDC" in s: continue
                
                # Analyze
                current_symbol_dfs[s] = strat.analyze(df, params=params)
            
            fold_scores = []
            fold_metrics = [] # To store details
            
            # --- Walk Forward (4 Folds) ---
            for f_idx, (start_dt, end_dt) in enumerate(folds):
                # Run Backtest
                res = bt.run_portfolio(current_symbol_dfs, params, start_date=start_dt, end_date=end_dt, verbose=False)
                
                # Metric Extraction
                ret = res['total_return']
                dd = 0.0
                trades_count = res['trades']
                win_rate = res['win_rate']
                
                # Calculate Max DD from trade list (Approximation)
                if res['trade_list']:
                    df_t = pd.DataFrame(res['trade_list'])
                    if 'max_dd' in df_t.columns:
                        dd = df_t['max_dd'].min() # Max DD is usually negative
                
                # Dynamic Min Trades
                min_req = self.calculate_min_trades(start_dt, end_dt)
                
                if trades_count < min_req:
                    score = -999.0 # Disqualify
                else:
                    score = self.calculate_score(ret, dd, trades_count, win_rate)
                
                fold_scores.append(score)
                fold_metrics.append({
                    'fold': f_idx,
                    'return': ret,
                    'trades': trades_count,
                    'win_rate': win_rate,
                    'max_dd': dd,
                    'score': score, 
                    'min_req': min_req
                })
                
                # Feedback
                current_step += 1
                if callback:
                    prog = current_step / total_steps
                    callback(prog, f"Trial {t_idx+1}/{len(trial_params_list)} (Fold {f_idx+1})")
            
            # --- Aggregate Score ---
            # Spec: score_final = 0.7*mean(score_fold) + 0.3*min(score_fold)
            final_score = 0.7 * np.mean(fold_scores) + 0.3 * np.min(fold_scores)
            
            # SSOT: Run ONE Full Backtest for the artifacts?
            # Or merge fold results? Merging is hard with state. 
            # Ideally, we verify the "Best" by running Full Backtest.
            # But we need to save artifacts for EVERY trial?
            # "trial_0000/ ... SSOT ÏÇ∞Ï∂úÎ¨º 4Ï¢Ö"
            # It seems we should run full backtest once per trial to generate artifacts.
            # Running 4 folds AND 1 Full = 5 runs per trial.
            # If speed is OK.
            
            # Running Full for Artifacts
            full_res = bt.run_portfolio(current_symbol_dfs, params, verbose=False)
            
            # Check constraints (Diagnosis)
            diagnosis = []
            if full_res['trades'] < 10: diagnosis.append("LowTrades")
            if full_res['total_return'] == 0: diagnosis.append("NoReturn")
            
            # Save Artifacts
            pd.DataFrame(full_res['trade_list']).to_csv(os.path.join(trial_dir, "final_trades.csv"), index=False, encoding='utf-8-sig')
            pd.DataFrame(full_res['event_list']).to_csv(os.path.join(trial_dir, "events.csv"), index=False, encoding='utf-8-sig')
            
            # Debug candidates log is HUGE. Maybe zip it or optimize? 
            # For now save simple JSON or CSV?
            # It's a dict date->list.
            # Let's skip saving heavy daily_debug for all trials (disk safeguard). 
            # Only save for Top? Or save separate per date?
            # User said "Output ... SSOT ... candidates_debug".
            # Let's save it. Maybe pickle or json.
            # with open(os.path.join(trial_dir, "daily_debug.json"), "w") as f:
            #     # Convert dates to str
            #     serializable_debug = {k.strftime('%Y-%m-%d'): v for k, v in full_res['daily_debug'].items()}
            #     json.dump(serializable_debug, f)
            # Avoiding huge IO for now unless critical.
            
            # Summary Result
            res_entry = {
                'trial_id': trial_id,
                'score': final_score,
                'total_return': full_res['total_return'],
                'trades': full_res['trades'],
                'win_rate': full_res['win_rate'],
                'params': params,
                'diagnosis': diagnosis,
                'fold_metrics': fold_metrics
            }
            results.append(res_entry)
            
        # 4. Finalize Run
        # Leaderboard
        df_res = pd.DataFrame(results)
        df_res = df_res.sort_values('score', ascending=False)
        df_res.to_csv(os.path.join(run_dir, "leaderboard.csv"), index=False)
        
        # Best Params
        best_trial = df_res.iloc[0]
        best_params = best_trial['params']
        with open(os.path.join(run_dir, "best_params.json"), "w") as f:
            json.dump(best_params, f, indent=4)
            
        # --- Backup Logic (Requested) ---
        backup_dir = os.path.join(self.output_dir, "backups")
        os.makedirs(backup_dir, exist_ok=True) # Defensive code as requested
        
        completion_time = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = os.path.join(backup_dir, f"best_params_{completion_time}.json")
        with open(backup_file, "w") as f:
            json.dump(best_params, f, indent=4)
            
        # --- Terminal Output (Requested) ---
        print("\n" + "="*50)
        print(f"‚úÖ Optimization Complete! Run ID: {run_id}")
        print(f"üìä Best Score: {best_trial['score']:.4f}")
        print(f"üìÇ Backup Saved: {backup_file}")
        print("-" * 50)
        print("üèÜ Best Parameters:")
        print(json.dumps(best_params, indent=4))
        print("="*50 + "\n")

        # Next Params (Top 10)
        top10 = df_res.head(10)[['trial_id', 'score', 'total_return']]
        top10.to_csv(os.path.join(run_dir, "next_params.csv"), index=False)
        
        return run_dir
